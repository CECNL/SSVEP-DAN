# -*- coding: utf-8 -*-
"""trca.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16pPN-CqWkATqNdc4yUUGbu5V4fSY86c-
"""

"""TRCA utils."""
import numpy as np
import warnings
#import matplotlib.pyplot as plt

from numpy.lib.stride_tricks import as_strided
from scipy.signal import filtfilt, cheb1ord, cheby1
from scipy import stats
from scipy import signal
import scipy.linalg as linalg
from scipy import io

def round_half_up(num, decimals=0):
    """Round half up round the last decimal of the number.
    The rules are:
    from 0 to 4 rounds down
    from 5 to 9 rounds up
    Parameters
    ----------
    num : float
        Number to round
    decimals : number of decimals
    Returns
    -------
    num rounded
    """
    multiplier = 10 ** decimals
    return int(np.floor(num * multiplier + 0.5) / multiplier)


def normfit(data, ci=0.95):
    """Compute the mean, std and confidence interval for them.
    Parameters
    ----------
    data : array, shape=()
        Input data.
    ci : float
        Confidence interval (default=0.95).
    Returns
    -------
    m : float
        Mean.
    sigma : float
        Standard deviation
    [m - h, m + h] : list
        Confidence interval of the mean.
    [sigmaCI_lower, sigmaCI_upper] : list
        Confidence interval of the std.
    """
    arr = 1.0 * np.array(data)
    num = len(arr)
    avg, std_err = np.mean(arr), stats.sem(arr)
    h_int = std_err * stats.t.ppf((1 + ci) / 2., num - 1)
    var = np.var(data, ddof=1)
    var_ci_upper = var * (num - 1) / stats.chi2.ppf((1 - ci) / 2, num - 1)
    var_ci_lower = var * (num - 1) / stats.chi2.ppf(1 - (1 - ci) / 2, num - 1)
    sigma = np.sqrt(var)
    sigma_ci_lower = np.sqrt(var_ci_lower)
    sigma_ci_upper = np.sqrt(var_ci_upper)

    return avg, sigma, [avg - h_int, avg +
                        h_int], [sigma_ci_lower, sigma_ci_upper]


def itr(n, p, t):
    """Compute information transfer rate (ITR).
    Definition in [1]_.
    Parameters
    ----------
    n : int
        Number of targets.
    p : float
        Target identification accuracy (0 <= p <= 1).
    t : float
        Average time for a selection (s).
    Returns
    -------
    itr : float
        Information transfer rate [bits/min]
    References
    ----------
    .. [1] M. Cheng, X. Gao, S. Gao, and D. Xu,
        "Design and Implementation of a Brain-Computer Interface With High
        Transfer Rates", IEEE Trans. Biomed. Eng. 49, 1181-1186, 2002.
    """
    itr = 0

    if (p < 0 or 1 < p):
        raise ValueError('Accuracy need to be between 0 and 1.')
    elif (p < 1 / n):
        itr = 0
        raise ValueError('ITR might be incorrect because accuracy < chance')
    elif (p == 1):
        itr = np.log2(n) * 60 / t
    else:
        itr = (np.log2(n) + p * np.log2(p) + (1 - p) *
               np.log2((1 - p) / (n - 1))) * 60 / t

    return itr


def bandpass(eeg, sfreq, Wp, Ws):
    """Filter bank design for decomposing EEG data into sub-band components.
    Parameters
    ----------
    eeg : np.array, shape=(n_samples, n_chans[, n_trials])
        Training data.
    sfreq : int
        Sampling frequency of the data.
    Wp : 2-tuple
        Passband for Chebyshev filter.
    Ws : 2-tuple
        Stopband for Chebyshev filter.
    Returns
    -------
    y: np.array, shape=(n_trials, n_chans, n_samples)
        Sub-band components decomposed by a filter bank.
    See Also
    --------
    scipy.signal.cheb1ord :
        Chebyshev type I filter order selection.
    """
    # Chebyshev type I filter order selection.
    N, Wn = cheb1ord(Wp, Ws, 3, 40, fs=sfreq)

    # Chebyshev type I filter design
    B, A = cheby1(N, 0.5, Wn, btype="bandpass", fs=sfreq) #lowpass bandpass
    
    # print('lenA:',len(A))
    # print('lenB:',len(B))
    
    # the arguments 'axis=0, padtype='odd', padlen=3*(max(len(B),len(A))-1)'
    # correspond to Matlab filtfilt : https://dsp.stackexchange.com/a/47945
    
    y = filtfilt(B, A, eeg, axis=0, padtype='odd',
                 padlen=3 * (max(len(B), len(A)) - 1))
    
    # y = filtfilt(B, A, eeg, axis=0)
    return y


def schaefer_strimmer_cov(X):
    """Schaefer-Strimmer covariance estimator.
    Shrinkage estimator described in [1]_:
    .. math:: \hat{\Sigma} = (1 - \gamma)\Sigma_{scm} + \gamma T
    where :math:`T` is the diagonal target matrix:
    .. math:: T_{i,j} = \{ \Sigma_{scm}^{ii} \text{if} i = j,
         0 \text{otherwise} \}
    Note that the optimal :math:`\gamma` is estimated by the authors' method.
    Parameters
    ----------
    X: array, shape=(n_chans, n_samples)
        Signal matrix.
    Returns
    -------
    cov: array, shape=(n_chans, n_chans)
        Schaefer-Strimmer shrinkage covariance matrix.
    References
    ----------
    .. [1] Schafer, J., and K. Strimmer. 2005. A shrinkage approach to
       large-scale covariance estimation and implications for functional
       genomics. Statist. Appl. Genet. Mol. Biol. 4:32.
    """
    ns = X.shape[1]
    C_scm = np.cov(X, ddof=0)
    X_c = X - np.tile(X.mean(axis=1), [ns, 1]).T

    # Compute optimal gamma, the weigthing between SCM and srinkage estimator
    R = ns / (ns - 1.0) * np.corrcoef(X)
    var_R = (X_c ** 2).dot((X_c ** 2).T) - 2 * C_scm * X_c.dot(X_c.T)
    var_R += ns * C_scm ** 2

    var_R = ns / ((ns - 1) ** 3 * np.outer(X.var(1), X.var(1))) * var_R
    R -= np.diag(np.diag(R))
    var_R -= np.diag(np.diag(var_R))
    gamma = max(0, min(1, var_R.sum() / (R ** 2).sum()))

    cov = (1. - gamma) * (ns / (ns - 1.)) * C_scm
    cov += gamma * (ns / (ns - 1.)) * np.diag(np.diag(C_scm))

    return cov


def _check_data(X):
    """Check data is numpy array and has the proper dimensions."""
    if not isinstance(X, (np.ndarray, list)):
        raise AttributeError('data should be a list or a numpy array')

    dtype = np.complex128 if np.any(np.iscomplex(X)) else np.float64
    X = np.asanyarray(X, dtype=dtype)
    if X.ndim > 3:
        raise ValueError('Data must be 3D at most')

    return X

def theshapeof(X):
    """Return the shape of X."""
    # X = _check_data(X)
    # if not isinstance(X, np.ndarray):
    #     raise AttributeError('X must be a numpy array')

    if X.ndim == 3:
        return X.shape[0], X.shape[1], X.shape[2]
    elif X.ndim == 2:
        return X.shape[0], X.shape[1], 1
    elif X.ndim == 1:
        return X.shape[0], 1, 1
    else:
        raise ValueError("Array contains more than 3 dimensions")

# For wearable dataset
def data_transformation(notch_freq,quality_factor,samp_freq,eeg): # 50,35,250 original:[8, 710, 10, 12]
    n_chan, n_sample, n_blocks, n_stimulus = eeg.shape
    new_data = np.zeros((n_chan,n_sample,n_blocks,n_stimulus))
    b_notch, a_notch = signal.iirnotch(notch_freq, quality_factor, samp_freq)
    for block in range(n_blocks):
        for trial in range(n_stimulus):
            for channel in range(n_chan):
                data = eeg[channel,:,block,trial]
                notchdata = signal.filtfilt(b_notch, a_notch, data)
                new_data[channel,:,block,trial] = notchdata
    new_data = new_data.transpose([0,1,3,2])
    return new_data

#################################### cross valid training ###################################
def train_valid_for_benchmark(i_sub, n_sub,supplementary_sub, training_ratio, path, channel_num, n_tps):
    random_select1 = int(supplementary_sub * training_ratio)
    random_select2 = supplementary_sub - random_select1
    sub_arr = np.arange(n_sub)
    # load data
    # target domain
    s2_i = i_sub+1
    s2 = path + str(s2_i) + '.mat'
    sample2 = io.loadmat(s2)
    target = np.array(sample2['data'])[channel_num,:,:,:]
    existing = target[:,:,:,:n_tps]
    # random select
    # training
    training_sub = sub_arr
    training_sub = np.delete(training_sub, i_sub, 0)
    random_sub_train = np.random.choice(training_sub.shape[0],random_select1, replace=False)
    training_sub = training_sub[random_sub_train]
    # validation
    valid_sub = sub_arr
    Del_data = np.append(training_sub,i_sub)
    valid_sub = np.delete(valid_sub, Del_data, 0)
    random_sub_valid = np.random.choice(valid_sub.shape[0],random_select2, replace=False)
    valid_sub = valid_sub[random_sub_valid]
    # concatanate all existing domain data for training
    for i in training_sub:
        if (i+1) != s2_i:
            s1_i = i+1
            s1 = path+str(s1_i)+'.mat'
            sample1 = io.loadmat(s1)
            existing_tmp = np.array(sample1['data'])[channel_num,:,:,:]
            # existing_tmp = existing_tmp[:,:,:,rand_indx]
            existing_tmp = existing_tmp[:,:,:,:n_tps]
            existing = np.concatenate((existing,existing_tmp),3)
    existing_train = existing[:,:,:,n_tps:]
    # concatanate all existing domain data for validation
    existing = target[:,:,:,:n_tps]
    for j in valid_sub:
        if (j+1) != s2_i:
            s1_i = j+1
            s1 = path+str(s1_i)+'.mat'
            sample1 = io.loadmat(s1)
            existing_tmp = np.array(sample1['data'])[channel_num,:,:,:]
            # existing_tmp = existing_tmp[:,:,:,rand_indx]
            existing_tmp = existing_tmp[:,:,:,:n_tps]
            existing = np.concatenate((existing,existing_tmp),3)
    existing_valid = existing[:,:,:,n_tps:]
    return existing_train, existing_valid, target

# CS: dev_ex == dev_tr , CD: dev_ex != dev_tr
def train_valid_for_wearable(i_sub, n_sub,supplementary_sub, training_ratio, path, dev_ex, dev_tr, n_tps):
    random_select1 = int(supplementary_sub * training_ratio)
    random_select2 = supplementary_sub - random_select1
    sub_arr = np.arange(n_sub)
    # load data
    # target domain
    s2_i = i_sub+1
    if s2_i < 10:
        s2 = path + 'S00' + str(s2_i)
    elif s2_i >= 10 and s2_i < 100:
        s2 = path + 'S0' + str(s2_i)
    else:
        s2 = path + 'S' + str(s2_i)
    sample2 = io.loadmat(s2)
    target = np.array(sample2['data'])[:,:,dev_tr,:,:] # channel,sample,trials,stimulus
    target = data_transformation(50,35,250,target) # channel,sample,stimulus,trials
    existing = target[:,:,:,:n_tps]
    # random select
    # training
    training_sub = sub_arr
    training_sub = np.delete(training_sub, i_sub, 0)
    random_sub_train = np.random.choice(training_sub.shape[0],random_select1, replace=False)
    training_sub = training_sub[random_sub_train]
    # validation
    valid_sub = sub_arr
    Del_data = np.append(training_sub,i_sub)
    valid_sub = np.delete(valid_sub, Del_data, 0)
    random_sub_valid = np.random.choice(valid_sub.shape[0],random_select2, replace=False)
    valid_sub = valid_sub[random_sub_valid]
    # concatanate all existing domain data for training
    for i in training_sub:
        if (i+1) != s2_i:
            s1_i = i+1
            if s1_i < 10:
                s1 = path + 'S00' + str(s1_i)
            elif s1_i >= 10 and s1_i < 100:
                s1 = path + 'S0' + str(s1_i)
            else:
                s1 = path + 'S' + str(s1_i)
            sample1 = io.loadmat(s1)
            existing_tmp = np.array(sample1['data'])[:,:,dev_ex,:,:]
            existing_tmp = data_transformation(50,35,250,existing_tmp) # channel,sample,stimulus,trials
            existing_tmp = existing_tmp[:,:,:,:n_tps]
            existing = np.concatenate((existing,existing_tmp),3)
    existing_train = existing[:,:,:,n_tps:]
    # concatanate all existing domain data for validation
    existing = target[:,:,:,:n_tps]
    for j in valid_sub:
        if (j+1) != s2_i:
            s1_i = j+1
            if s1_i < 10:
                s1 = path + 'S00' + str(s1_i)
            elif s1_i >= 10 and s1_i < 100:
                s1 = path + 'S0' + str(s1_i)
            else:
                s1 = path + 'S' + str(s1_i)
            sample1 = io.loadmat(s1)
            existing_tmp = np.array(sample1['data'])[:,:,dev_ex,:,:]
            existing_tmp = data_transformation(50,35,250,existing_tmp) # channel,sample,stimulus,trials
            existing_tmp = existing_tmp[:,:,:,:n_tps]
            existing = np.concatenate((existing,existing_tmp),3)
    existing_valid = existing[:,:,:,n_tps:]
    return existing_train, existing_valid, target